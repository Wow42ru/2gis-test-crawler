Задание
---
Simple crawler.

Необходимо создать простой интернет crawler, который будет доставать из страниц информацию о названии сайта. Это должно
быть приложение с http эндпоинтом. На этот эндпоинт поступает список http url'ов. Приложение должно пройтись по всем
предоставленным урлам и достать оттуда название. Названием условно будем считать содержимое тэга title. После изъятия
информации из всех страниц эндпоинт должен вернуть ответ, в котором каждому входному урлу соответствует найденное
название. Все недостающие требования или неоднозначности начальной формулировки задачи вы должны разрешить
самостоятельно - это является частью задания. Единственное требование к реализации - приложение должно быть написано на
языке Scala.

## Решение

Решено, что обработка запроса будет выполняться синхронно, то есть запрос будет блокироваться до тех пор, пока не будет
завершен парсинг всей информации по всем URL-адресам, полученным в запросе.

###Формат входных данных: 
URL передаются в виде JSON-массива в теле POST-запроса к эндпоинту `/titles`.

```
curl -X POST \
-H 'Content-Type: application/json' \
-d '["https://www.wikipedia.org", "https://www.2gis.ru"]' \
http://localhost:8080/titles
```
###Формат ответа
Ответ — это JSON-массив, где каждому входному URL соответствует структура с информацией о названии, коде состояния 
и возможной ошибке. Если URL недоступен или невалиден, эндпоинт возвращает URL c кодом состояния и 
сообщением об ошибке вместо названия. Успешному поиску названия соответствует код 200.
####Пример ответа

```
{
  "results": [
    {
      "url": "https://www.wikipedia.org",
      "status": 200,
      "result": "Wikipedia"
    },
    {
      "url": "https://www.2gis.ru",
      "status": 301,
      "result": "301 Moved Permanently"
    }
  ]
}
```

 ## Техническая реализация

Приложение написано в функциональном стиле в typelevel стеке и состоит из двух основных частей:

* `Crawler`: Компонент, отвечающий за скачивание веб-страниц и извлечение заголовков. принимает на вход список Url, парсинг происходит параллельно.
* `ApiApp`: Серверная часть, которая принимает запросы на скачивание заголовков, обрабатывает их и возвращает результат.

1. `ApiApp` обрабатывает входящие POST-запросы к ендпоинту `/titles`.
2. Извлекая список URL из тела запроса, `ApiApp` передает его `Crawler` для обработки.
3. `Crawler` использует HTTP-клиент для парсинга страниц и извлечения заголовков, работая в параллельном режиме.
4. После завершения сканирования `Crawler` возвращает список объектов `TitleResponse` обратно в `ApiApp`.
5. `ApiApp` преобразует результат в JSON с помощью `JsonSerializer` и отправляет его клиенту в качестве ответа.

