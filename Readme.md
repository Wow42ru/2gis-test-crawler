## Задание
---
Simple crawler.

Необходимо создать простой интернет crawler, который будет доставать из страниц информацию о названии сайта. Это должно
быть приложение с http эндпоинтом. На этот эндпоинт поступает список http url'ов. Приложение должно пройтись по всем
предоставленным урлам и достать оттуда название. Названием условно будем считать содержимое тэга title. После изъятия
информации из всех страниц эндпоинт должен вернуть ответ, в котором каждому входному урлу соответствует найденное
название. Все недостающие требования или неоднозначности начальной формулировки задачи вы должны разрешить
самостоятельно - это является частью задания. Единственное требование к реализации - приложение должно быть написано на
языке Scala.

## Решение

Решено, что обработка запроса будет выполняться синхронно, то есть запрос будет блокироваться до тех пор, пока не будет
завершен парсинг всей информации по всем URL-адресам, полученным в запросе.

Формат входных данных: URL передаются в виде JSON-массива в теле POST-запроса к эндпоинту /titles.

```
curl -X POST \
-H 'Content-Type: application/json' \
-d '["https://www.wikipedia.org", "https://www.2gis.ru"]' \
http://localhost:8080/titles
```
Ответ — это JSON-массив, где каждому входному URL соответствует структура с информацией о названии, коде состояния 
и возможной ошибке. Если URL недоступен или невалиден, эндпоинт возвращает url c кодом состояния и 
сообщением об ошибке вместо названия. Успешному поиску названия соответствует код 200.
Пример ответа

```
{
  "results": [
    {
      "url": "https://www.wikipedia.org",
      "status": 200,
      "result": "Wikipedia"
    },
    {
      "url": "https://www.2gis.ru",
      "status": 301,
      "result": "301 Moved Permanently"
    }
  ]
}
```

 ## Техническая реализация

Приложение написано в функциональном стиле в typelevel стеке и состоит из двух основных частей:

* Crawler: Компонент, отвечающий за скачивание веб-страниц и извлечение заголовков. принимает на вход список Url, парсинг происходит параллельно.
* ApiApp: Серверная часть, которая принимает запросы на скачивание заголовков, обрабатывает их и возвращает результат.